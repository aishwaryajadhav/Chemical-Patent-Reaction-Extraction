{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from dataset  import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"test_data_iob.csv\"\n",
    "val_file = \"val_data_iob.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda =  True  with num_workers =  8  system version =  3.7.13 (default, Oct 18 2022, 18:57:03) \n",
      "[GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = \"recobo/chemical-bert-uncased-pharmaceutical-chemical-classifier\"\n",
    "batch_size = 4\n",
    "max_para_length = 128\n",
    "para_seq_len = 16  #number of paras to be encoded and decoded together (hyperparameter)\n",
    "# Check if cuda is available and set device\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "# Make sure you choose suitable num_worker, otherwise it will result in errors\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(\"Cuda = \", str(cuda), \" with num_workers = \", str(num_workers),  \" system version = \", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "test_data = CRFEmbeddingDataset(test_file, para_seq_len = para_seq_len, pretrained_model = pretrained_model, stride = para_seq_len)\n",
    "test_args = dict(shuffle=False, batch_size=batch_size, num_workers=8, pin_memory=True, drop_last=False) if cuda else dict(shuffle=False, batch_size=batch_size, drop_last=False)\n",
    "test_loader = DataLoader(test_data, **test_args)\n",
    "\n",
    "val_data = CRFEmbeddingDataset(val_file, para_seq_len = para_seq_len, pretrained_model = pretrained_model, stride = para_seq_len)\n",
    "val_args = dict(shuffle=False, batch_size=batch_size, num_workers=8, pin_memory=True, drop_last=False) if cuda else dict(shuffle=False, batch_size=batch_size, drop_last=False)\n",
    "val_loader = DataLoader(val_data, **val_args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471\n",
      "804\n",
      "201\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(val_data.__len__())\n",
    "print(test_data.__len__())\n",
    "\n",
    "print(len(test_loader))\n",
    "print(len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1\n",
    "./chem_bert_iob_bilstm_crf_bert_finetune/enc_dec_model_model_params_0.9525667530577686.pth\n",
    "\n",
    "pretrained_model = \"recobo/chemical-bert-uncased-pharmaceutical-chemical-classifier\"\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "max_para_length = 128\n",
    "\n",
    "para_seq_len = 16  #number of paras to be encoded and decoded together (hyperparameter)\n",
    "\n",
    "'./model_model_params_0.9428545098368426.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at recobo/chemical-bert-uncased-pharmaceutical-chemical-classifier were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params:  130909458\n",
      "Trainable params:  130909458\n",
      "Non Trainable params:  0\n"
     ]
    }
   ],
   "source": [
    "model = EncoderDecoderBiLstmCRF(embed_model = BertEmbedding(pretrained_model), num_tags = 3, freeze_bert=False)\n",
    "model.load_state_dict(torch.load('./chem_bert_iob_bilstm_crf_bert_finetune/enc_dec_model_model_params_0.9525667530577686.pth'))\n",
    "# model = load_pretrained_weights(model, './model_model_params_0.9428545098368426.pth')\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#   print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#   model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "non_trainable_total_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "print(\"Total params: \", total_params)\n",
    "print(\"Trainable params: \", trainable_total_params)\n",
    "print(\"Non Trainable params: \", non_trainable_total_params)\n",
    "\n",
    "\n",
    "epochs = 10 #changed from 10\n",
    "lamda = 1e-3  #L2 regularization (prev : 1e-4)\n",
    "learning_rate = 5e-5 #changed from 1e-2   ## Greatly reduces LR for bert finetuning\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# criterion = criterion.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=lamda)\n",
    "# optimizer.load_state_dict(torch.load('./bert_base_triplet/optimizer_model_params_0.9409211846833226.pth'))    \n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[i for i in range(4,20,4)], gamma=0.75)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(test_loader) * epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: [5e-05]\n",
      "Validation loss: 4.65, Time: 17.07770848274231\n",
      "{'eval_accuracy': 0.9532908704883227, 'eval_precision': 0.952948854512343, 'eval_recall': 0.9532908704883227, 'eval_f-1': 0.9521363436342459}\n",
      "Total original spans:  911\n",
      "Total predicted spans:  758\n",
      "Total number of original spans correctly predicted acc to strict match:  672\n",
      "Percent of original spans correctly predicted acc to strict match:  73.76509330406147\n",
      "Total number of original spans correctly predicted acc to fuzzy match:  729\n",
      "Percent of original spans correctly predicted acc to fuzzy match:  80.02195389681668\n",
      "Count of fuzzy matched spans:  57\n",
      "Count of spans with misaligned begin and end: 21 (36.84%) \n",
      "Count of spans with misaligned begin: 15 (26.32%) \n",
      "Count of spans with misaligned end: 21 (36.84%) \n"
     ]
    }
   ],
   "source": [
    "val_df = pd.read_csv(val_file)\n",
    "_, val_predictions = validate(model, val_loader, device, optimizer, scheduler)\n",
    "_ = get_span_perf(val_df, val_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: [5e-05]\n",
      "Validation loss: 5.04, Time: 29.1264865398407\n",
      "{'eval_accuracy': 0.9406094527363185, 'eval_precision': 0.9394594020800763, 'eval_recall': 0.9406094527363185, 'eval_f-1': 0.9397341370157223}\n",
      "Total original spans:  1234\n",
      "Total predicted spans:  1104\n",
      "Total number of original spans correctly predicted acc to strict match:  854\n",
      "Percent of original spans correctly predicted acc to strict match:  69.20583468395462\n",
      "Total number of original spans correctly predicted acc to fuzzy match:  962\n",
      "Percent of original spans correctly predicted acc to fuzzy match:  77.9578606158833\n",
      "Count of fuzzy matched spans:  108\n",
      "Count of spans with misaligned begin and end: 13 (12.04%) \n",
      "Count of spans with misaligned begin: 26 (24.07%) \n",
      "Count of spans with misaligned end: 69 (63.89%) \n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_file)\n",
    "_, test_predictions = validate(model, test_loader, device, optimizer, scheduler)\n",
    "_ = get_span_perf(test_df, test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2\n",
    "\n",
    "./chem_bert_iob_bilstm_crf_bert_finetune/enc_dec_model_model_params_0.953476708995007.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at recobo/chemical-bert-uncased-pharmaceutical-chemical-classifier were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params:  130909458\n",
      "Trainable params:  130909458\n",
      "Non Trainable params:  0\n"
     ]
    }
   ],
   "source": [
    "model = EncoderDecoderBiLstmCRF(embed_model = BertEmbedding(pretrained_model), num_tags = 3, freeze_bert=False)\n",
    "model.load_state_dict(torch.load('./chem_bert_iob_bilstm_crf_bert_finetune/enc_dec_model_model_params_0.953476708995007.pth'))\n",
    "# model = load_pretrained_weights(model, './model_model_params_0.9428545098368426.pth')\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#   print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#   model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "non_trainable_total_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "print(\"Total params: \", total_params)\n",
    "print(\"Trainable params: \", trainable_total_params)\n",
    "print(\"Non Trainable params: \", non_trainable_total_params)\n",
    "\n",
    "\n",
    "epochs = 10 #changed from 10\n",
    "lamda = 1e-3  #L2 regularization (prev : 1e-4)\n",
    "learning_rate = 5e-5 #changed from 1e-2   ## Greatly reduces LR for bert finetuning\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# criterion = criterion.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=lamda)\n",
    "# optimizer.load_state_dict(torch.load('./bert_base_triplet/optimizer_model_params_0.9409211846833226.pth'))    \n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[i for i in range(4,20,4)], gamma=0.75)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(test_loader) * epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: [5e-05]\n",
      "Validation loss: 2.80, Time: 16.683980464935303\n",
      "{'eval_accuracy': 0.9538216560509554, 'eval_precision': 0.9534230759626194, 'eval_recall': 0.9538216560509554, 'eval_f-1': 0.953476708995007}\n",
      "Total original spans:  911\n",
      "Total predicted spans:  861\n",
      "Total number of original spans correctly predicted acc to strict match:  706\n",
      "Percent of original spans correctly predicted acc to strict match:  77.49725576289791\n",
      "Total number of original spans correctly predicted acc to fuzzy match:  764\n",
      "Percent of original spans correctly predicted acc to fuzzy match:  83.86388583973655\n",
      "Count of fuzzy matched spans:  58\n",
      "Count of spans with misaligned begin and end: 28 (48.28%) \n",
      "Count of spans with misaligned begin: 14 (24.14%) \n",
      "Count of spans with misaligned end: 16 (27.59%) \n"
     ]
    }
   ],
   "source": [
    "val_df = pd.read_csv(val_file)\n",
    "_, val_predictions = validate(model, val_loader, device, optimizer, scheduler)\n",
    "_ = get_span_perf(val_df, val_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: [5e-05]\n",
      "Validation loss: 4.07, Time: 28.625789642333984\n",
      "{'eval_accuracy': 0.9465174129353234, 'eval_precision': 0.945344867870555, 'eval_recall': 0.9465174129353234, 'eval_f-1': 0.9455013765218424}\n",
      "Total original spans:  1234\n",
      "Total predicted spans:  1102\n",
      "Total number of original spans correctly predicted acc to strict match:  874\n",
      "Percent of original spans correctly predicted acc to strict match:  70.82658022690438\n",
      "Total number of original spans correctly predicted acc to fuzzy match:  993\n",
      "Percent of original spans correctly predicted acc to fuzzy match:  80.47001620745543\n",
      "Count of fuzzy matched spans:  119\n",
      "Count of spans with misaligned begin and end: 23 (19.33%) \n",
      "Count of spans with misaligned begin: 31 (26.05%) \n",
      "Count of spans with misaligned end: 65 (54.62%) \n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_file)\n",
    "_, test_predictions = validate(model, test_loader, device, optimizer, scheduler)\n",
    "_ = get_span_perf(test_df, test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3\n",
    "\n",
    "/home/anjadhav/Chemical-Patent-Reaction-Extraction/chem_bert_iob_bilstm_crf_bert_finetune/model_model_params_0.9521363436342459.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at recobo/chemical-bert-uncased-pharmaceutical-chemical-classifier were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params:  130909458\n",
      "Trainable params:  130909458\n",
      "Non Trainable params:  0\n"
     ]
    }
   ],
   "source": [
    "model = EncoderDecoderBiLstmCRF(embed_model = BertEmbedding(pretrained_model), num_tags = 3, freeze_bert=False)\n",
    "model.load_state_dict(torch.load('./chem_bert_iob_bilstm_crf_bert_finetune/model_model_params_0.9521363436342459.pth'))\n",
    "# model = load_pretrained_weights(model, './model_model_params_0.9428545098368426.pth')\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#   print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#   model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "non_trainable_total_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "print(\"Total params: \", total_params)\n",
    "print(\"Trainable params: \", trainable_total_params)\n",
    "print(\"Non Trainable params: \", non_trainable_total_params)\n",
    "\n",
    "\n",
    "epochs = 10 #changed from 10\n",
    "lamda = 1e-3  #L2 regularization (prev : 1e-4)\n",
    "learning_rate = 5e-5 #changed from 1e-2   ## Greatly reduces LR for bert finetuning\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# criterion = criterion.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=lamda)\n",
    "# optimizer.load_state_dict(torch.load('./bert_base_triplet/optimizer_model_params_0.9409211846833226.pth'))    \n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[i for i in range(4,20,4)], gamma=0.75)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(test_loader) * epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: [5e-05]\n",
      "Validation loss: 4.65, Time: 17.00141930580139\n",
      "{'eval_accuracy': 0.9532908704883227, 'eval_precision': 0.952948854512343, 'eval_recall': 0.9532908704883227, 'eval_f-1': 0.9521363436342459}\n",
      "Total original spans:  911\n",
      "Total predicted spans:  758\n",
      "Total number of original spans correctly predicted acc to strict match:  672\n",
      "Percent of original spans correctly predicted acc to strict match:  73.76509330406147\n",
      "Total number of original spans correctly predicted acc to fuzzy match:  729\n",
      "Percent of original spans correctly predicted acc to fuzzy match:  80.02195389681668\n",
      "Count of fuzzy matched spans:  57\n",
      "Count of spans with misaligned begin and end: 21 (36.84%) \n",
      "Count of spans with misaligned begin: 15 (26.32%) \n",
      "Count of spans with misaligned end: 21 (36.84%) \n"
     ]
    }
   ],
   "source": [
    "val_df = pd.read_csv(val_file)\n",
    "_, val_predictions = validate(model, val_loader, device, optimizer, scheduler)\n",
    "_ = get_span_perf(val_df, val_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: [5e-05]\n",
      "Validation loss: 5.04, Time: 28.995968103408813\n",
      "{'eval_accuracy': 0.9406094527363185, 'eval_precision': 0.9394594020800763, 'eval_recall': 0.9406094527363185, 'eval_f-1': 0.9397341370157223}\n",
      "Total original spans:  1234\n",
      "Total predicted spans:  1104\n",
      "Total number of original spans correctly predicted acc to strict match:  854\n",
      "Percent of original spans correctly predicted acc to strict match:  69.20583468395462\n",
      "Total number of original spans correctly predicted acc to fuzzy match:  962\n",
      "Percent of original spans correctly predicted acc to fuzzy match:  77.9578606158833\n",
      "Count of fuzzy matched spans:  108\n",
      "Count of spans with misaligned begin and end: 13 (12.04%) \n",
      "Count of spans with misaligned begin: 26 (24.07%) \n",
      "Count of spans with misaligned end: 69 (63.89%) \n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_file)\n",
    "_, test_predictions = validate(model, test_loader, device, optimizer, scheduler)\n",
    "_ = get_span_perf(test_df, test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4\n",
    "/home/anjadhav/Chemical-Patent-Reaction-Extraction/chem_bert_iob_bilstm_crf/model_model_params_0.9539811224849719.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoderBiLstmCRF(embed_model = BertEmbedding(pretrained_model), num_tags = 3, freeze_bert=False)\n",
    "model.load_state_dict(torch.load('./chem_bert_iob_bilstm_crf/model_model_params_0.9539811224849719.pth'))\n",
    "# model = load_pretrained_weights(model, './model_model_params_0.9428545098368426.pth')\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#   print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#   model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "non_trainable_total_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "print(\"Total params: \", total_params)\n",
    "print(\"Trainable params: \", trainable_total_params)\n",
    "print(\"Non Trainable params: \", non_trainable_total_params)\n",
    "\n",
    "\n",
    "epochs = 10 #changed from 10\n",
    "lamda = 1e-3  #L2 regularization (prev : 1e-4)\n",
    "learning_rate = 5e-5 #changed from 1e-2   ## Greatly reduces LR for bert finetuning\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# criterion = criterion.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=lamda)\n",
    "# optimizer.load_state_dict(torch.load('./bert_base_triplet/optimizer_model_params_0.9409211846833226.pth'))    \n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[i for i in range(4,20,4)], gamma=0.75)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(test_loader) * epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   6%|▌         | 7/118 [00:01<00:17,  6.44it/s]"
     ]
    }
   ],
   "source": [
    "val_df = pd.read_csv(val_file)\n",
    "_, val_predictions = validate(model, val_loader, device, optimizer, scheduler)\n",
    "_ = get_span_perf(val_df, val_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('chemIR': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bca01836d6106e973a10514ba84a22def982a0caf4526f342e9e0f31ef9581b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
