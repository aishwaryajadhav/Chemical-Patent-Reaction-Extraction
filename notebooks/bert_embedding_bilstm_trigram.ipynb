{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2O1OB_Nw-DXo",
    "outputId": "710381bd-f9ab-4808-ab30-f307a3c93dfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/envs/pytorch/lib/python3.9/site-packages (2.5.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/envs/pytorch/lib/python3.9/site-packages (4.22.2)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (4.63.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (1.23.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (0.10.0)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (2022.8.2)\n",
      "Requirement already satisfied: dill<0.3.6 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers) (2022.9.13)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from pandas->datasets) (2022.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Wb1LntOk-HiL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import nltk\n",
    "import random\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# from nltk.corpus import stopwords\n",
    "# english_stopwords = stopwords.words(\"english\")\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import string\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import multiprocessing\n",
    "import time\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XPPwJGc7-rG6"
   },
   "outputs": [],
   "source": [
    "test_file = \"dev_data.csv\"\n",
    "val_file = \"val_data.csv\"\n",
    "train_file = \"train_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T5WVHoZVR-mO",
    "outputId": "e30f28b3-6493-4874-86e5-4b4341e0ea3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda =  True  with num_workers =  8  system version =  3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) \n",
      "[GCC 10.3.0]\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = \"bert-base-uncased\"\n",
    "batch_size = 128\n",
    "max_para_length = 128\n",
    "\n",
    "# Check if cuda is available and set device\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "# Make sure you choose suitable num_worker, otherwise it will result in errors\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(\"Cuda = \", str(cuda), \" with num_workers = \", str(num_workers),  \" system version = \", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HwFvdeKHZQhW"
   },
   "outputs": [],
   "source": [
    "class ContextEmbeddingDataset(Dataset):\n",
    "    def __init__(self, csv_file, context, pretrained_model):\n",
    "      df = pd.read_csv(csv_file)\n",
    "\n",
    "      self.context = context\n",
    "      self.tokenizer = BertTokenizer.from_pretrained(pretrained_model, do_lower=True)    \n",
    "\n",
    "      # Tokenize the paragraphs\n",
    "      self.df = df[\"para\"].apply(self.preprocess)\n",
    "      self.y = df['label']\n",
    "  \n",
    "     \n",
    "    def preprocess(self, examples):\n",
    "      return self.tokenizer(examples, truncation=True, \n",
    "                     padding=\"max_length\", max_length=max_para_length,\n",
    "                     return_token_type_ids=False)['input_ids']\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.y) - (2*self.context)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "      return torch.LongTensor(list(self.df[index:(index + 2*self.context+1)])), self.y[index+self.context]\n",
    "      \n",
    "      # self.embed_model.eval()\n",
    "      # Generate BERT embeddings for the tokens in each para\n",
    "      # with torch.no_grad():\n",
    "      #   x = torch.LongTensor(list(self.df[index:(index + 2*self.context+1)])).to(device)\n",
    "      #   print(x.shape)    \n",
    "      #   outputs = self.embed_model(x)\n",
    "      #   print(outputs.shape) # (3, tokens(128), input_dim(3072))\n",
    "       \n",
    "      # return outputs.cpu(), self.y[index+self.context]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GeKJeI1DnZSt",
    "outputId": "3154a924-0105-4b60-d9ee-d4256fac5be1"
   },
   "outputs": [],
   "source": [
    "val_data = ContextEmbeddingDataset(val_file, context = 1, pretrained_model = pretrained_model)\n",
    "train_data = ContextEmbeddingDataset(train_file, context = 1, pretrained_model = pretrained_model)\n",
    "test_data = ContextEmbeddingDataset(test_file, context = 1, pretrained_model = pretrained_model)\n",
    "\n",
    "train_args = dict(shuffle=True, batch_size=batch_size, num_workers=8, pin_memory=True, drop_last=False) if cuda else dict(shuffle=True, batch_size=batch_size, drop_last=False)\n",
    "train_loader = DataLoader(train_data, **train_args)\n",
    "\n",
    "val_args = dict(shuffle=False, batch_size=batch_size, num_workers=8, pin_memory=True, drop_last=False) if cuda else dict(shuffle=False, batch_size=batch_size, drop_last=False)\n",
    "val_loader = DataLoader(val_data, **val_args)\n",
    "\n",
    "\n",
    "test_args = dict(shuffle=False, batch_size=batch_size, num_workers=8, pin_memory=True, drop_last=False) if cuda else dict(shuffle=False, batch_size=batch_size, drop_last=False)\n",
    "test_loader = DataLoader(test_data, **test_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svcSRucHTZs9"
   },
   "source": [
    "## Fixed Bert word Embeddings, BiLSTM encoder, Triplet Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6_zxBd--j0vk"
   },
   "outputs": [],
   "source": [
    "class BertEmbedding(nn.Module):\n",
    "  def __init__(self, pretrained_model):\n",
    "    super().__init__()\n",
    "    self.model = BertModel.from_pretrained(pretrained_model, output_hidden_states = True)\n",
    "    # for param in self.model.bert.parameters():\n",
    "    #   param.requires_grad = False\n",
    "    # print(sum(p.numel() for p in self.model.parameters()))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # print(\"Input to BertEmbedding: \", x.shape)\n",
    "    outputs = self.model(x)\n",
    "    hidden_states = outputs[2]\n",
    "    embedding = torch.cat((hidden_states[-1],hidden_states[-2],hidden_states[-3],hidden_states[-4]), dim = 2)\n",
    "    # print(\"Output from BertEmbedding: \", embedding.shape)\n",
    "    return embedding\n",
    "\n",
    "class ParaEncoderForContext(nn.Module):\n",
    "  def __init__(self, bilayers = 1, input_dim = 3072, hidden_size = 512):\n",
    "    super().__init__()\n",
    "    self.input_dim = input_dim\n",
    "    self.hidden_dim = hidden_size\n",
    "    self.lstm = nn.LSTM(\n",
    "            input_size=input_dim, hidden_size=hidden_size,\n",
    "            num_layers=1, batch_first=True, bidirectional=True)\n",
    "    \n",
    "    for name, param in self.lstm.named_parameters():\n",
    "      if 'bias' in name:\n",
    "        nn.init.constant(param, 0.0)\n",
    "      elif 'weight' in name:\n",
    "        nn.init.orthogonal(param)\n",
    "     \n",
    "\n",
    "  def forward(self, x): # (B*T(T=1+2*context), tokens, input_dim)\n",
    "    # print(\"Input to Encoder: \",x.shape)\n",
    "    outputs, _ = self.lstm(x) # (B*T, tokens, 2*hidden_dim)\n",
    "    # print(\"After LSTM: \", outputs.shape)\n",
    "    first = outputs[:, 0, self.hidden_dim:]\n",
    "    second = outputs[:, -1, :self.hidden_dim]\n",
    "    para_embed = torch.cat((second,first), dim = 1) #(B*T, 2*hidden_dim)\n",
    "\n",
    "    # print(\"Output from Encoder\", para_embed.shape)\n",
    "    return para_embed #(B*T, 2*hidden_dim)\n",
    "\n",
    "\n",
    "\n",
    "class ParaDecoderTriplet(nn.Module):\n",
    "  def __init__(self, input_size, output_size = 1):\n",
    "    super().__init__()\n",
    "    self.linear = nn.Linear(input_size, 1, bias= True);\n",
    "    # self.layers = nn.Sequential(nn.Linear(input_size, output_size, bias = True), \n",
    "    #                             nn.BatchNorm1d(output_size), \n",
    "    #                             nn.ReLU(inplace = True), \n",
    "    #                             nn.Linear(output_size, 1, bias = True))\n",
    "    \n",
    "    # for mod in self.modules():\n",
    "    #   if isinstance(mod, nn.BatchNorm1d):\n",
    "    #     nn.init.constant_(mod.weight.data, 1)\n",
    "    #     if(mod.bias is not None):\n",
    "    #       nn.init.constant_(mod.bias.data, 0)\n",
    "\n",
    "  def forward(self, x): # #(B, T, 2*hidden_dim)\n",
    "    # print(\"Input to decoder: \", x.shape) \n",
    "    s0,s1,s2 = x.shape\n",
    "    x = x.reshape(s0,-1) #concat main and context para embeddings\n",
    "    # print(\"Input to linear layer in decoder:\", xv.shape) \n",
    "    return self.linear(x) #(B,1)\n",
    "\n",
    "class EncoderDecoderTriplet(nn.Module):\n",
    "  def __init__(self, embed_model, decoder_output_size = 1, encoder_bilayers = 1, encoder_input_dim = 3072, encoder_hidden_size = 512, context = 1):\n",
    "    super().__init__()\n",
    "    self.para_encoder = ParaEncoderForContext(bilayers = encoder_bilayers, input_dim = encoder_input_dim, hidden_size = encoder_hidden_size)\n",
    "    self.para_decoder = ParaDecoderTriplet(input_size = encoder_hidden_size*2*(1+2*context))\n",
    "    self.embed_model = embed_model\n",
    "    #freeze bert embedding layer\n",
    "    for param in self.embed_model.parameters():\n",
    "      param.requires_grad = False\n",
    "\n",
    "  def forward(self, x): # (B, 2*context+1, tokens_per_para)\n",
    "    # print(\"Input to model: \", x.shape)\n",
    "    s0, s1, s2 = x.shape\n",
    "    xv = x.view(s0*s1, s2)\n",
    "    embeds = self.embed_model(xv)\n",
    "    para_vec = self.para_encoder(embeds)\n",
    "    pvv = para_vec.view(s0, s1, -1)\n",
    "    # print(\"Input to decoder: \", pvv.shape)\n",
    "    return self.para_decoder(pvv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxePIGPBZefb"
   },
   "source": [
    "## Train and Validate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "R661ZBw5TJ3v"
   },
   "outputs": [],
   "source": [
    "def train(para_model, data_loader):\n",
    "  para_model.train()\n",
    "\n",
    "  avg_loss = []\n",
    "  all_predictions = []\n",
    "  all_targets = []\n",
    "  start = time.time()\n",
    "\n",
    "  for i, (x, y) in enumerate(tqdm(data_loader, desc=\"Epoch\", leave=False)):\n",
    "    optimizer.zero_grad()\n",
    "    y  = y.to(device) \n",
    "    x = x.to(device)\n",
    " \n",
    "    output = para_model(x)\n",
    "\n",
    "    # print(\"Output from model: \", output.shape)  \n",
    "\n",
    "    loss = criterion(torch.squeeze(output), y.float())\n",
    "    avg_loss.extend([loss.item()]*len(y))\n",
    "\n",
    "    output = nn.Sigmoid()(output)\n",
    "\n",
    "    all_predictions.extend((output >= 0.5).tolist())\n",
    "    all_targets.extend(y.tolist())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    \n",
    "  end = time.time()\n",
    "  avg_loss = np.mean(avg_loss)\n",
    "  print('learning_rate: {}'.format(scheduler.get_last_lr()))\n",
    "  print('Training loss: {:.2f}, Time: {}'.format(avg_loss, end-start))\n",
    "  \n",
    "  all_predictions = np.array(all_predictions)\n",
    "  all_targets = np.array(all_targets)\n",
    "  scores = precision_recall_fscore_support(all_targets, all_predictions, \n",
    "                                            average=\"weighted\", zero_division=0.)\n",
    "  \n",
    "  test_scores={\n",
    "      \"eval_accuracy\": (all_predictions == all_targets).sum() / len(all_predictions),\n",
    "      \"eval_precision\": scores[0],\n",
    "      \"eval_recall\": scores[1],\n",
    "      \"eval_f-1\": scores[2]\n",
    "  }\n",
    "  print(test_scores)\n",
    "  return test_scores[\"eval_f-1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "_y3ZyfMCW6A8"
   },
   "outputs": [],
   "source": [
    "def validate(para_model, data_loader):\n",
    "  para_model.eval()\n",
    "  \n",
    "  avg_loss = []\n",
    "  all_predictions = []\n",
    "  all_targets = []\n",
    "  start = time.time()\n",
    "\n",
    "  for i, (x, y) in enumerate(tqdm(data_loader, desc=\"Epoch\", leave=False)):\n",
    "    # optimizer.zero_grad()\n",
    "\n",
    "    y = y.to(device)\n",
    "    x = x.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      output = para_model(x)\n",
    "\n",
    "      loss = criterion(torch.squeeze(output), y.float())\n",
    "      avg_loss.extend([loss.item()]*len(y))\n",
    "\n",
    "      output = nn.Sigmoid()(output)\n",
    "\n",
    "      all_predictions.extend((output >= 0.5).tolist())\n",
    "      all_targets.extend(y.tolist())\n",
    "    \n",
    "\n",
    "    \n",
    "  end = time.time()\n",
    "  avg_loss = np.mean(avg_loss)\n",
    "  print('learning_rate: {}'.format(scheduler.get_last_lr()))\n",
    "  print('Validation loss: {:.2f}, Time: {}'.format(avg_loss, end-start))\n",
    "  \n",
    "  all_predictions = np.array(all_predictions)\n",
    "  all_targets = np.array(all_targets)\n",
    "  scores = precision_recall_fscore_support(all_targets, all_predictions, \n",
    "                                            average=\"weighted\", zero_division=0.)\n",
    "  \n",
    "  test_scores={\n",
    "      \"eval_accuracy\": (all_predictions == all_targets).sum() / len(all_predictions),\n",
    "      \"eval_precision\": scores[0],\n",
    "      \"eval_recall\": scores[1],\n",
    "      \"eval_f-1\": scores[2]\n",
    "  }\n",
    "  print(test_scores)\n",
    "  return test_scores[\"eval_f-1\"], all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "0bgq32FRSK3P"
   },
   "outputs": [],
   "source": [
    "def save(model, acc, best=\"\"):\n",
    "    if not os.path.exists('./bert_base_triplet/'):\n",
    "        os.mkdir('./bert_base_triplet/')\n",
    "\n",
    "    torch.save(model.state_dict(), './bert_base_triplet/'+'/{}model_params_{}.pth'.format(best, acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKb52nsGZllu"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uN6Pfp1_YyVl",
    "outputId": "4d59b235-ca6e-411f-a607-91592b5a7906"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params:  124173569\n",
      "Trainable params:  14691329\n",
      "Non Trainable params:  109482240\n"
     ]
    }
   ],
   "source": [
    "model = EncoderDecoderTriplet(embed_model = BertEmbedding(pretrained_model))\n",
    "model.load_state_dict(torch.load('./bert_base_triplet/model_model_params_0.9463038575553862.pth'))\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "non_trainable_total_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "print(\"Total params: \", total_params)\n",
    "print(\"Trainable params: \", trainable_total_params)\n",
    "print(\"Non Trainable params: \", non_trainable_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Apmgr6gLTH3X"
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "lamda = 1e-4  #L2 regularization\n",
    "learning_rate = 0.01\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=lamda)\n",
    "# optimizer.load_state_dict(torch.load('./bert_base_triplet/model_model_params_0.9463038575553862.pth'))    \n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[i for i in range(4,20,4)], gamma=0.75)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ApcG3QO1Vel2",
    "outputId": "b5cd2679-a0cd-4469-c595-341d5b6944ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: [0.009938441702975686]\n",
      "Training loss: 0.14, Time: 1282.683581352234\n",
      "{'eval_accuracy': 27794.043785214853, 'eval_precision': 0.9436408794746415, 'eval_recall': 0.9438372592944584, 'eval_f-1': 0.9437294348809004}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: [0.009938441702975686]\n",
      "Validation loss: 0.22, Time: 218.57729744911194\n",
      "{'eval_accuracy': 5455.185137732223, 'eval_precision': 0.9216137421868582, 'eval_recall': 0.9143711296177663, 'eval_f-1': 0.9161539592897534}\n",
      "Epoch #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: [0.00024471741852423256]\n",
      "Training loss: 0.06, Time: 1355.2685902118683\n",
      "{'eval_accuracy': 27710.720521349536, 'eval_precision': 0.9812671260994843, 'eval_recall': 0.981241373068133, 'eval_f-1': 0.9812531334618015}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: [0.00024471741852423256]\n",
      "Validation loss: 0.20, Time: 200.84149169921875\n",
      "{'eval_accuracy': 5589.968182788811, 'eval_precision': 0.9431553240609312, 'eval_recall': 0.9422378816997651, 'eval_f-1': 0.942571255932125}\n",
      "Epoch #19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: [6.155829702431176e-05]\n",
      "Training loss: 0.05, Time: 1326.984935760498\n",
      "{'eval_accuracy': 27702.13570022402, 'eval_precision': 0.9834701128404973, 'eval_recall': 0.9834362908152875, 'eval_f-1': 0.9834509620053837}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: [6.155829702431176e-05]\n",
      "Validation loss: 0.20, Time: 217.57310342788696\n",
      "{'eval_accuracy': 5588.615844544096, 'eval_precision': 0.9447732479954126, 'eval_recall': 0.9438394191757421, 'eval_f-1': 0.9441730126427105}\n",
      "Epoch #20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: [0.0]\n",
      "Training loss: 0.05, Time: 1324.8539867401123\n",
      "{'eval_accuracy': 27706.680605525762, 'eval_precision': 0.9846166028923713, 'eval_recall': 0.9845903197338945, 'eval_f-1': 0.9846018586348805}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: [0.0]\n",
      "Validation loss: 0.20, Time: 216.82944703102112\n",
      "{'eval_accuracy': 5585.911168054666, 'eval_precision': 0.9439990602925357, 'eval_recall': 0.942985265855221, 'eval_f-1': 0.9433430824981578}\n"
     ]
    }
   ],
   "source": [
    "best_val_f1 = 0\n",
    "for epoch in range(epochs):\n",
    "  print('Epoch #{}'.format(epoch+1))\n",
    "  \n",
    "  train_f1 = train(model, train_loader)\n",
    "  val_f1 = validate(model, val_loader)\n",
    "  \n",
    "  if val_f1 > best_val_f1:\n",
    "    best_val_f1 = val_f1\n",
    "    save(model, best_val_f1, best = \"model_\")\n",
    "    save(optimizer, best_val_f1, best = \"optimizer_\")\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ymyNg6tRaCEi"
   },
   "outputs": [],
   "source": [
    "# Test on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: [0.01]\n",
      "Validation loss: 0.25, Time: 283.2222058773041\n",
      "{'eval_accuracy': 8525.18222084078, 'eval_precision': 0.912120269830713, 'eval_recall': 0.9139793301732846, 'eval_f-1': 0.9113673431309153}\n"
     ]
    }
   ],
   "source": [
    "_, predictions = validate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = predictions.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12869\n",
      "12871\n"
     ]
    }
   ],
   "source": [
    "temp = predictions\n",
    "print(len(predictions))\n",
    "predictions = np.concatenate([[0], p, [0]])\n",
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9139926967601585\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_file)\n",
    "test_df['predictions'] = predictions\n",
    "error = test_df[test_df['label'] != test_df['predictions']]\n",
    "print((len(test_df)- len(error)) / len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(error))\n",
    "error.to_csv(\"errors_bert_embed_triplet_binary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
