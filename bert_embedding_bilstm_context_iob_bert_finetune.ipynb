{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2O1OB_Nw-DXo",
    "outputId": "710381bd-f9ab-4808-ab30-f307a3c93dfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/envs/pytorch/lib/python3.9/site-packages (2.5.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/envs/pytorch/lib/python3.9/site-packages (4.22.2)\n",
      "Requirement already satisfied: nltk in /opt/conda/envs/pytorch/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: xxhash in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (0.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: dill<0.3.6 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (1.23.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (4.63.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (2022.8.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers) (2022.9.13)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from pandas->datasets) (2022.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/pytorch/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Wb1LntOk-HiL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import nltk\n",
    "import random\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# from nltk.corpus import stopwords\n",
    "# english_stopwords = stopwords.words(\"english\")\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import string\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import multiprocessing\n",
    "import time\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XPPwJGc7-rG6"
   },
   "outputs": [],
   "source": [
    "test_file = \"test_data_iob.csv\"\n",
    "val_file = \"val_data_iob.csv\"\n",
    "train_file = \"train_data_iob.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T5WVHoZVR-mO",
    "outputId": "e30f28b3-6493-4874-86e5-4b4341e0ea3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda =  True  with num_workers =  8  system version =  3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21) \n",
      "[GCC 10.3.0]\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = \"bert-base-uncased\"\n",
    "batch_size = 128\n",
    "max_para_length = 128\n",
    "\n",
    "# Check if cuda is available and set device\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "# Make sure you choose suitable num_worker, otherwise it will result in errors\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(\"Cuda = \", str(cuda), \" with num_workers = \", str(num_workers),  \" system version = \", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HwFvdeKHZQhW"
   },
   "outputs": [],
   "source": [
    "class ContextEmbeddingDataset(Dataset):\n",
    "    def __init__(self, csv_file, context, pretrained_model):\n",
    "      df = pd.read_csv(csv_file)\n",
    "\n",
    "      self.context = context\n",
    "      self.tokenizer = BertTokenizer.from_pretrained(pretrained_model, do_lower=True)    \n",
    "\n",
    "      # Tokenize the paragraphs\n",
    "      self.df = df[\"para\"].apply(self.preprocess)\n",
    "      self.y = df['label']\n",
    "  \n",
    "     \n",
    "    def preprocess(self, examples):\n",
    "      return self.tokenizer(examples, truncation=True, \n",
    "                     padding=\"max_length\", max_length=max_para_length,\n",
    "                     return_token_type_ids=False)['input_ids']\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.y) - (2*self.context)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "      return torch.LongTensor(list(self.df[index:(index + 2*self.context+1)])), self.y[index+self.context]\n",
    "      \n",
    "      # self.embed_model.eval()\n",
    "      # Generate BERT embeddings for the tokens in each para\n",
    "      # with torch.no_grad():\n",
    "      #   x = torch.LongTensor(list(self.df[index:(index + 2*self.context+1)])).to(device)\n",
    "      #   print(x.shape)    \n",
    "      #   outputs = self.embed_model(x)\n",
    "      #   print(outputs.shape) # (3, tokens(128), input_dim(3072))\n",
    "       \n",
    "      # return outputs.cpu(), self.y[index+self.context]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GeKJeI1DnZSt",
    "outputId": "3154a924-0105-4b60-d9ee-d4256fac5be1"
   },
   "outputs": [],
   "source": [
    "val_data = ContextEmbeddingDataset(val_file, context = 1, pretrained_model = pretrained_model)\n",
    "train_data = ContextEmbeddingDataset(train_file, context = 1, pretrained_model = pretrained_model)\n",
    "test_data = ContextEmbeddingDataset(test_file, context = 1, pretrained_model = pretrained_model)\n",
    "\n",
    "train_args = dict(shuffle=True, batch_size=batch_size, num_workers=8, pin_memory=True, drop_last=False) if cuda else dict(shuffle=True, batch_size=batch_size, drop_last=False)\n",
    "train_loader = DataLoader(train_data, **train_args)\n",
    "\n",
    "val_args = dict(shuffle=False, batch_size=batch_size, num_workers=8, pin_memory=True, drop_last=False) if cuda else dict(shuffle=False, batch_size=batch_size, drop_last=False)\n",
    "val_loader = DataLoader(val_data, **val_args)\n",
    "\n",
    "\n",
    "test_args = dict(shuffle=False, batch_size=batch_size, num_workers=8, pin_memory=True, drop_last=False) if cuda else dict(shuffle=False, batch_size=batch_size, drop_last=False)\n",
    "test_loader = DataLoader(test_data, **test_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svcSRucHTZs9"
   },
   "source": [
    "## Fixed Bert word Embeddings, BiLSTM encoder, Triplet Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6_zxBd--j0vk"
   },
   "outputs": [],
   "source": [
    "class BertEmbedding(nn.Module):\n",
    "  def __init__(self, pretrained_model):\n",
    "    super().__init__()\n",
    "    self.model = BertModel.from_pretrained(pretrained_model, output_hidden_states = True)\n",
    "  \n",
    "\n",
    "  def forward(self, x):\n",
    "    # print(\"Input to BertEmbedding: \", x.shape)\n",
    "    outputs = self.model(x)\n",
    "    hidden_states = outputs[2]\n",
    "    embedding = torch.cat((hidden_states[-1],hidden_states[-2],hidden_states[-3],hidden_states[-4]), dim = 2)\n",
    "    # print(\"Output from BertEmbedding: \", embedding.shape)\n",
    "    return embedding\n",
    "\n",
    "class ParaEncoderForContext(nn.Module):\n",
    "  def __init__(self, bilayers = 1, input_dim = 3072, hidden_size = 512):\n",
    "    super().__init__()\n",
    "    self.input_dim = input_dim\n",
    "    self.hidden_dim = hidden_size\n",
    "    self.lstm = nn.LSTM(\n",
    "            input_size=input_dim, hidden_size=hidden_size,\n",
    "            num_layers=1, batch_first=True, bidirectional=True)\n",
    "    \n",
    "    for name, param in self.lstm.named_parameters():\n",
    "      if 'bias' in name:\n",
    "        nn.init.constant(param, 0.0)\n",
    "      elif 'weight' in name:\n",
    "        nn.init.orthogonal(param)\n",
    "     \n",
    "\n",
    "  def forward(self, x): # (B*T(T=1+2*context), tokens, input_dim)\n",
    "    # print(\"Input to Encoder: \",x.shape)\n",
    "    outputs, _ = self.lstm(x) # (B*T, tokens, 2*hidden_dim)\n",
    "    # print(\"After LSTM: \", outputs.shape)\n",
    "    first = outputs[:, 0, self.hidden_dim:]\n",
    "    second = outputs[:, -1, :self.hidden_dim]\n",
    "    para_embed = torch.cat((second,first), dim = 1) #(B*T, 2*hidden_dim)\n",
    "\n",
    "    # print(\"Output from Encoder\", para_embed.shape)\n",
    "    return para_embed #(B*T, 2*hidden_dim)\n",
    "\n",
    "\n",
    "\n",
    "class ParaDecoderTriplet(nn.Module):\n",
    "  def __init__(self, input_size, output_size = 1):\n",
    "    super().__init__()\n",
    "    self.linear = nn.Linear(input_size, 3, bias= True)\n",
    "    # self.layers = nn.Sequential(nn.Linear(input_size, output_size, bias = True), \n",
    "    #                             nn.BatchNorm1d(output_size), \n",
    "    #                             nn.ReLU(inplace = True), \n",
    "    #                             nn.Linear(output_size, 1, bias = True))\n",
    "    \n",
    "    # for mod in self.modules():\n",
    "    #   if isinstance(mod, nn.BatchNorm1d):\n",
    "    #     nn.init.constant_(mod.weight.data, 1)\n",
    "    #     if(mod.bias is not None):\n",
    "    #       nn.init.constant_(mod.bias.data, 0)\n",
    "\n",
    "  def forward(self, x): # #(B, T, 2*hidden_dim)\n",
    "    # print(\"Input to decoder: \", x.shape) \n",
    "    s0,s1,s2 = x.shape\n",
    "    x = x.reshape(s0,-1) #concat main and context para embeddings\n",
    "    # print(\"Input to linear layer in decoder:\", xv.shape) \n",
    "    return self.linear(x) #(B,1)\n",
    "\n",
    "class EncoderDecoderTriplet(nn.Module):\n",
    "  def __init__(self, embed_model, decoder_output_size = 1, encoder_bilayers = 1, encoder_input_dim = 3072, encoder_hidden_size = 512, context = 1, freeze_bert = True):\n",
    "    super().__init__()\n",
    "    self.para_encoder = ParaEncoderForContext(bilayers = encoder_bilayers, input_dim = encoder_input_dim, hidden_size = encoder_hidden_size)\n",
    "    self.para_decoder = ParaDecoderTriplet(input_size = encoder_hidden_size*2*(1+2*context))\n",
    "    self.embed_model = embed_model\n",
    "    #NO freeze bert embedding layer\n",
    "    if (freeze_bert):\n",
    "        for param in self.embed_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "  def forward(self, x): # (B, 2*context+1, tokens_per_para)\n",
    "    # print(\"Input to model: \", x.shape)\n",
    "    s0, s1, s2 = x.shape\n",
    "    xv = x.view(s0*s1, s2)\n",
    "    embeds = self.embed_model(xv)\n",
    "    para_vec = self.para_encoder(embeds)\n",
    "    pvv = para_vec.view(s0, s1, -1)\n",
    "    # print(\"Input to decoder: \", pvv.shape)\n",
    "    return self.para_decoder(pvv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxePIGPBZefb"
   },
   "source": [
    "## Train and Validate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "R661ZBw5TJ3v"
   },
   "outputs": [],
   "source": [
    "def train(para_model, data_loader):\n",
    "  para_model.train()\n",
    "\n",
    "  avg_loss = []\n",
    "  all_predictions = []\n",
    "  all_targets = []\n",
    "  start = time.time()\n",
    "\n",
    "  for i, (x, y) in enumerate(tqdm(data_loader, desc=\"Epoch\", leave=False)):\n",
    "    optimizer.zero_grad()\n",
    "    y  = y.to(device) \n",
    "    x = x.to(device)\n",
    " \n",
    "    output = para_model(x)\n",
    "\n",
    "    # print(\"Output from model: \", output.shape)  \n",
    "\n",
    "    loss = criterion(output, y.long())\n",
    "    avg_loss.extend([loss.item()]*len(y))\n",
    "\n",
    "    # output = nn.Sigmoid()(output)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    all_predictions.extend(torch.argmax(output.detach(), axis=1).cpu().tolist())\n",
    "    all_targets.extend(y.detach().cpu().tolist())\n",
    "     \n",
    "    \n",
    "  end = time.time()\n",
    "  avg_loss = np.mean(avg_loss)\n",
    "  print('learning_rate: {}'.format(scheduler.get_last_lr()))\n",
    "  print('Training loss: {:.2f}, Time: {}'.format(avg_loss, end-start))\n",
    "  \n",
    "  all_predictions = np.array(all_predictions)\n",
    "  all_targets = np.array(all_targets)\n",
    "  scores = precision_recall_fscore_support(all_targets, all_predictions, \n",
    "                                            average=\"weighted\", zero_division=0.)\n",
    "  \n",
    "  test_scores={\n",
    "      \"eval_accuracy\": (all_predictions == all_targets).sum() / len(all_predictions),\n",
    "      \"eval_precision\": scores[0],\n",
    "      \"eval_recall\": scores[1],\n",
    "      \"eval_f-1\": scores[2]\n",
    "  }\n",
    "  print(test_scores)\n",
    "  return test_scores[\"eval_f-1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_y3ZyfMCW6A8"
   },
   "outputs": [],
   "source": [
    "def validate(para_model, data_loader):\n",
    "  para_model.eval()\n",
    "  \n",
    "  avg_loss = []\n",
    "  all_predictions = []\n",
    "  all_targets = []\n",
    "  start = time.time()\n",
    "\n",
    "  for i, (x, y) in enumerate(tqdm(data_loader, desc=\"Epoch\", leave=False)):\n",
    "    # optimizer.zero_grad()\n",
    "\n",
    "    y = y.to(device)\n",
    "    x = x.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      output = para_model(x)\n",
    "\n",
    "      loss = criterion(output, y.long())\n",
    "      avg_loss.extend([loss.item()]*len(y))\n",
    "\n",
    "      # output = nn.Sigmoid()(output)\n",
    "\n",
    "      all_predictions.extend(torch.argmax(output.detach(), axis=1).cpu().tolist())\n",
    "      all_targets.extend(y.detach().cpu().tolist())\n",
    "\n",
    "    \n",
    "  end = time.time()\n",
    "  avg_loss = np.mean(avg_loss)\n",
    "  print('learning_rate: {}'.format(scheduler.get_last_lr()))\n",
    "  print('Validation loss: {:.2f}, Time: {}'.format(avg_loss, end-start))\n",
    "  \n",
    "  all_predictions = np.array(all_predictions)\n",
    "  all_targets = np.array(all_targets)\n",
    "  scores = precision_recall_fscore_support(all_targets, all_predictions, \n",
    "                                            average=\"weighted\", zero_division=0.)\n",
    "  \n",
    "  test_scores={\n",
    "      \"eval_accuracy\": (all_predictions == all_targets).sum() / len(all_predictions),\n",
    "      \"eval_precision\": scores[0],\n",
    "      \"eval_recall\": scores[1],\n",
    "      \"eval_f-1\": scores[2]\n",
    "  }\n",
    "  print(test_scores)\n",
    "  return test_scores[\"eval_f-1\"], all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0bgq32FRSK3P"
   },
   "outputs": [],
   "source": [
    "def save(model, acc, best=\"\"):\n",
    "    if not os.path.exists('./bert_base_triplet_iob_bert_ft/'):\n",
    "        os.mkdir('./bert_base_triplet_iob_bert_ft/')\n",
    "\n",
    "    torch.save(model.state_dict(), './bert_base_triplet_iob_bert_ft/'+'/{}model_params_{}.pth'.format(best, acc))\n",
    "\n",
    "def load_pretrained_weights(model, pretrained_path):\n",
    "    pretrained_dict = torch.load(pretrained_path)\n",
    "#     pretrained_dict = {k: v for k, v in pretrained_dict.items() if k[:13] == \"para_encoder.\"}\n",
    "    # print(pretrained_dict.keys())\n",
    "    model_dict = model.state_dict()\n",
    "    model_dict.update(pretrained_dict) \n",
    "    model.load_state_dict(model_dict)\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKb52nsGZllu"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uN6Pfp1_YyVl",
    "outputId": "4d59b235-ca6e-411f-a607-91592b5a7906"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params:  124179715\n",
      "Trainable params:  124179715\n",
      "Non Trainable params:  0\n"
     ]
    }
   ],
   "source": [
    "model = EncoderDecoderTriplet(embed_model = BertEmbedding(pretrained_model), freeze_bert = False)\n",
    "model = load_pretrained_weights(model, './bert_base_triplet_iob/model_model_params_0.9371448069961689.pth')\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "non_trainable_total_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "print(\"Total params: \", total_params)\n",
    "print(\"Trainable params: \", trainable_total_params)\n",
    "print(\"Non Trainable params: \", non_trainable_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Apmgr6gLTH3X"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lamda = 1e-3  #L2 regularization\n",
    "learning_rate = 5e-5 ## Greatly reduces LR for bert finetuning\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=lamda)\n",
    "# optimizer.load_state_dict(torch.load('./bert_base_triplet/optimizer_model_params_0.9409211846833226.pth'))    \n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[i for i in range(4,20,4)], gamma=0.75)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ApcG3QO1Vel2",
    "outputId": "b5cd2679-a0cd-4469-c595-341d5b6944ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 14.56 GiB total capacity; 8.72 GiB already allocated; 71.50 MiB free; 9.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      3\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch #\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m   train_f1 \u001b[38;5;241m=\u001b[39m train(model, train_loader)\n\u001b[1;32m      6\u001b[0m   val_f1, _ \u001b[38;5;241m=\u001b[39m validate(model, val_loader)\n\u001b[1;32m      8\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m val_f1 \u001b[38;5;241m>\u001b[39m best_val_f1:\n",
      "Cell \u001b[0;32mIn [15], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(para_model, data_loader)\u001b[0m\n\u001b[1;32m     11\u001b[0m y  \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device) \n\u001b[1;32m     12\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 14\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mpara_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# print(\"Output from model: \", output.shape)  \u001b[39;00m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, y\u001b[38;5;241m.\u001b[39mlong())\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [14], line 80\u001b[0m, in \u001b[0;36mEncoderDecoderTriplet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     78\u001b[0m s0, s1, s2 \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     79\u001b[0m xv \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(s0\u001b[38;5;241m*\u001b[39ms1, s2)\n\u001b[0;32m---> 80\u001b[0m embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m para_vec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpara_encoder(embeds)\n\u001b[1;32m     82\u001b[0m pvv \u001b[38;5;241m=\u001b[39m para_vec\u001b[38;5;241m.\u001b[39mview(s0, s1, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [14], line 9\u001b[0m, in \u001b[0;36mBertEmbedding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m      8\u001b[0m   \u001b[38;5;66;03m# print(\"Input to BertEmbedding: \", x.shape)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m   hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     11\u001b[0m   embedding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((hidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],hidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m],hidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m],hidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m]), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1013\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1015\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1016\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1017\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[0;32m-> 1022\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1035\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:611\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    602\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    603\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    604\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    608\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    609\u001b[0m     )\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 611\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    621\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    536\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    537\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 539\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/pytorch_utils.py:247\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 247\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:551\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 551\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:451\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 451\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 14.56 GiB total capacity; 8.72 GiB already allocated; 71.50 MiB free; 9.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "best_val_f1 = 0\n",
    "for epoch in range(epochs):\n",
    "  print('Epoch #{}'.format(epoch+1))\n",
    "  \n",
    "  train_f1 = train(model, train_loader)\n",
    "  val_f1, _ = validate(model, val_loader)\n",
    "  \n",
    "  if val_f1 > best_val_f1:\n",
    "    best_val_f1 = val_f1\n",
    "    save(model, best_val_f1, best = \"model_\")\n",
    "    save(optimizer, best_val_f1, best = \"optimizer_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymyNg6tRaCEi"
   },
   "outputs": [],
   "source": [
    "# Test on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predictions = validate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = predictions\n",
    "print(len(predictions))\n",
    "predictions = np.concatenate([[0], predictions, [0]])\n",
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_file)\n",
    "test_df['predictions'] = predictions\n",
    "test_df.to_csv(\"bert_embed_triplet_iob_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = test_df[test_df['label'] != test_df['predictions']]\n",
    "print((len(test_df)- len(error)) / len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(error))\n",
    "error.to_csv(\"errors_bert_embed_triplet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error \n",
    "# 1. errors are on table rows which contain chemical name independently (without a lot of other content) \n",
    "# just like chemical name headings for reaction starts\n",
    "# 2. Reaction headings that do not contain chemical names: EXAMPLE 3. Selective deprotection of position 6.\\n\n",
    "# 3. Errors on B tags (label 2) 3A: Sometimes paras that are like this:\n",
    "# Example 15: N-(5-(4-(5-bromo-3-methyl-2-oxo-2,3-dihydro-1H-benzo[d]imidazole -1-yl)pyrimidin-2-ylamino)-2-((2-(dimethylamino)ethyl)(methyl)amino)-4-methoxyphenyl) acrylamide hydrochloride\\n\n",
    "# are tagged as outer (0) or Beginning (2) in the gold standard. Model probably needs more context\n",
    "# 3B : Headings (tag B (2)) such as 1H NMR of GLP-111: DMSO-d6,  1.56-1.57 (br, m, 9H, 3CH2), 1.61-1.63 (br, m, 2H, CH2), 2.01 (brs, 4H, CH2), 2.36 (br, 1H, NHCH2), 3.22 (brs, 2H, CH2), 5.77 (s, 1H, OH), 6.44-6.46 (dd, 1H, Arom-H), 6.92-6.93 (dd, 1H, Arom-H) 7.06-7.09 (t, 1H, Arom-H), 7.21-7.22 (t, 1H, Arom-H), 9.40 (br, s, 1H, NH), 9.74 (br, s, 1H, NH).\\n\n",
    "# that contain properties of chemicals are classified as 0 because in most cases, the paras that just contain the properites\n",
    "# are not tagged as reaction paras\n",
    "# 4. Long paras containing a lot of chemical names are tagged as 2 even tho they are 0\n",
    "# 5. Example 4m\\n\t : tagged as 2 even tho they are 0\n",
    "# 6. Tables inside reactions are not recognized as reactions (tagged as 0 instead of 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
