{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2O1OB_Nw-DXo",
    "outputId": "710381bd-f9ab-4808-ab30-f307a3c93dfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (2.6.1)\n",
      "Requirement already satisfied: transformers in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (4.23.1)\n",
      "Requirement already satisfied: nltk in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (3.7)\n",
      "Requirement already satisfied: pytorch-crf in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (0.7.2)\n",
      "Requirement already satisfied: torch in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (1.13.0)\n",
      "Requirement already satisfied: seaborn in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (0.12.1)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (3.5.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from datasets) (0.10.1)\n",
      "Requirement already satisfied: packaging in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from datasets) (1.21.6)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from datasets) (2022.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from datasets) (4.11.4)\n",
      "Requirement already satisfied: responses<0.19 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pandas in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: multiprocess in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: dill<0.3.6 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: aiohttp in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: xxhash in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from datasets) (3.1.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from datasets) (10.0.0)\n",
      "Requirement already satisfied: filelock in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from transformers) (2022.9.13)\n",
      "Requirement already satisfied: click in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: setuptools in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.4.0)\n",
      "Requirement already satisfied: wheel in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.8/24.8 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from pandas->datasets) (2022.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.10.0)\n",
      "Collecting scipy>=1.1.0\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.1/38.1 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1304 sha256=1d50e2a04a051233330b47e8a83f8e9d78baeb353e89920d6873141f94aa0825\n",
      "  Stored in directory: /home/anjadhav/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn, sklearn\n",
      "Successfully installed scikit-learn-1.0.2 scipy-1.7.3 sklearn-0.0 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers nltk pytorch-crf torch seaborn sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Wb1LntOk-HiL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anjadhav/miniconda3/envs/chemIR/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchcrf import CRF\n",
    "import os\n",
    "import os.path as osp\n",
    "import nltk\n",
    "import random\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# from nltk.corpus import stopwords\n",
    "# english_stopwords = stopwords.words(\"english\")\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import string\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import multiprocessing\n",
    "import time\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XPPwJGc7-rG6"
   },
   "outputs": [],
   "source": [
    "test_file = \"test_data_iob.csv\"\n",
    "val_file = \"val_data_iob.csv\"\n",
    "train_file = \"train_data_iob.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T5WVHoZVR-mO",
    "outputId": "e30f28b3-6493-4874-86e5-4b4341e0ea3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda =  True  with num_workers =  8  system version =  3.7.13 (default, Oct 18 2022, 18:57:03) \n",
      "[GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = \"bert-base-uncased\"\n",
    "batch_size = 49\n",
    "max_para_length = 128\n",
    "para_seq_len = 16  #number of paras to be encoded and decoded together (hyperparameter)\n",
    "# Check if cuda is available and set device\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "# Make sure you choose suitable num_worker, otherwise it will result in errors\n",
    "num_workers = 8 if cuda else 0\n",
    "\n",
    "print(\"Cuda = \", str(cuda), \" with num_workers = \", str(num_workers),  \" system version = \", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HwFvdeKHZQhW"
   },
   "outputs": [],
   "source": [
    "class CRFEmbeddingDataset(Dataset):\n",
    "    def __init__(self, csv_file, para_seq_len, pretrained_model, stride = 1,  is_test=False):\n",
    "      df = pd.read_csv(csv_file)\n",
    "\n",
    "      self.para_seq_len = para_seq_len\n",
    "      self.tokenizer = BertTokenizer.from_pretrained(pretrained_model, do_lower=True)    \n",
    "\n",
    "      # Tokenize the paragraphs\n",
    "      self.df = df[\"para\"].apply(self.preprocess)\n",
    "      self.y = df['label']\n",
    "      # self.test = is_test\n",
    "      if(is_test):\n",
    "        self.stride = self.para_seq_len\n",
    "      else:\n",
    "        self.stride = stride\n",
    "  \n",
    "     \n",
    "    def preprocess(self, examples):\n",
    "      return self.tokenizer(examples, truncation=True, \n",
    "                     padding=\"max_length\", max_length=max_para_length,\n",
    "                     return_token_type_ids=False)['input_ids']\n",
    "\n",
    "    def __len__(self):\n",
    "      # if(self.test):\n",
    "      #   # print(math.ceil(len(self.y)/self.para_seq_len))\n",
    "      #   return math.ceil(len(self.y)/self.para_seq_len)\n",
    "      l = math.ceil((len(self.y) - self.para_seq_len + 1) / self.stride)\n",
    "      # print(len(self.y))  \n",
    "      # print(l)\n",
    "      return l\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "      return torch.LongTensor(list(self.df[index*self.stride: (index*self.stride + self.para_seq_len)])), torch.LongTensor(self.y[index*self.stride: (index*self.stride + self.para_seq_len)].tolist())\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GeKJeI1DnZSt",
    "outputId": "3154a924-0105-4b60-d9ee-d4256fac5be1"
   },
   "outputs": [],
   "source": [
    "train_data = CRFEmbeddingDataset(train_file, para_seq_len = para_seq_len, pretrained_model = pretrained_model, stride = 2)\n",
    "val_data = CRFEmbeddingDataset(val_file, para_seq_len = para_seq_len, pretrained_model = pretrained_model, is_test= True)\n",
    "test_data = CRFEmbeddingDataset(test_file, para_seq_len = para_seq_len, pretrained_model = pretrained_model, is_test= True)\n",
    "\n",
    "train_args = dict(shuffle=True, batch_size=batch_size, num_workers=8, pin_memory=True, drop_last=False) if cuda else dict(shuffle=True, batch_size=batch_size, drop_last=False)\n",
    "train_loader = DataLoader(train_data, **train_args)\n",
    "\n",
    "val_args = dict(shuffle=False, batch_size=batch_size, num_workers=8, pin_memory=True, drop_last=False) if cuda else dict(shuffle=False, batch_size=batch_size, drop_last=False)\n",
    "val_loader = DataLoader(val_data, **val_args)\n",
    "\n",
    "\n",
    "test_args = dict(shuffle=False, batch_size=batch_size, num_workers=8, pin_memory=True, drop_last=False) if cuda else dict(shuffle=False, batch_size=batch_size, drop_last=False)\n",
    "test_loader = DataLoader(test_data, **test_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svcSRucHTZs9"
   },
   "source": [
    "## Fixed Bert word Embeddings, BiLSTM encoder, Triplet Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6_zxBd--j0vk"
   },
   "outputs": [],
   "source": [
    "class BertEmbedding(nn.Module):\n",
    "  def __init__(self, pretrained_model):\n",
    "    super().__init__()\n",
    "    self.model = BertModel.from_pretrained(pretrained_model, output_hidden_states = True)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # print(\"Input to BertEmbedding: \", x.shape)\n",
    "    outputs = self.model(x)\n",
    "    hidden_states = outputs[2]\n",
    "    embedding = torch.cat((hidden_states[-1],hidden_states[-2],hidden_states[-3],hidden_states[-4]), dim = 2)\n",
    "    # print(\"Output from BertEmbedding: \", embedding.shape)\n",
    "    return embedding\n",
    "\n",
    "class ParaEncoderForContext(nn.Module):\n",
    "  def __init__(self, bilayers = 1, input_dim = 3072, hidden_size = 512):\n",
    "    super().__init__()\n",
    "    self.input_dim = input_dim\n",
    "    self.hidden_dim = hidden_size\n",
    "    self.lstm = nn.LSTM(\n",
    "            input_size=input_dim, hidden_size=hidden_size,\n",
    "            num_layers=1, batch_first=True, bidirectional=True)\n",
    "    \n",
    "    for name, param in self.lstm.named_parameters():\n",
    "      if 'bias' in name:\n",
    "        nn.init.constant(param, 0.0)\n",
    "      elif 'weight' in name:\n",
    "        nn.init.orthogonal(param)\n",
    "     \n",
    "\n",
    "  def forward(self, x): # (B*T(T=1+2*context), tokens, input_dim)\n",
    "    # print(\"Input to Encoder: \",x.shape)\n",
    "    outputs, _ = self.lstm(x) # (B*T, tokens, 2*hidden_dim)\n",
    "    # print(\"After LSTM: \", outputs.shape)\n",
    "    first = outputs[:, 0, self.hidden_dim:]\n",
    "    second = outputs[:, -1, :self.hidden_dim]\n",
    "    para_embed = torch.cat((second,first), dim = 1) #(B*T, 2*hidden_dim)\n",
    "\n",
    "    # print(\"Output from Encoder\", para_embed.shape)\n",
    "    return para_embed #(B*T, 2*hidden_dim)\n",
    "\n",
    "\n",
    "\n",
    "class ParaDecoderBiLstmCRF(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_size, bilayers = 1):\n",
    "    super().__init__()\n",
    "    # self.input_dim = input_dim\n",
    "    # self.hidden_dim = hidden_size\n",
    "    self.lstm = nn.LSTM(\n",
    "            input_size=input_dim, hidden_size=hidden_size,\n",
    "            num_layers=1, batch_first=True, bidirectional=True)\n",
    "    \n",
    "    for name, param in self.lstm.named_parameters():\n",
    "      if 'bias' in name:\n",
    "        nn.init.constant(param, 0.0)\n",
    "      elif 'weight' in name:\n",
    "        nn.init.orthogonal(param)\n",
    "     \n",
    "    self.linear = nn.Linear(2*hidden_size, 3, bias= True)\n",
    "    \n",
    "    \n",
    "  def forward(self, x):  #(B, T, 2*encoder.hidden_dim)\n",
    "    # print(\"Input to decoder: \", x.shape) \n",
    "    outputs, _ = self.lstm(x)   #out = (B, T, 2*decoder.hidden_dim)\n",
    "    \n",
    "    s0, s1, s2 = outputs.shape\n",
    "    op = outputs.reshape(s0*s1, s2) # (B*T, 2*decoder.hidden_dim)\n",
    "    \n",
    "    op2 = self.linear(op)\n",
    "    \n",
    "    op3 = op2.view(s0, s1, -1)\n",
    "    \n",
    "    return op3 #(B,T,3) #emissions\n",
    "\n",
    "\n",
    "\n",
    "class EncoderDecoderBiLstmCRF(nn.Module):\n",
    "  def __init__(self, embed_model, encoder_bilayers = 1, encoder_input_dim = 3072, encoder_hidden_size = 512, decoder_bilayers = 1, decoder_hidden_size = 512):\n",
    "    super().__init__()\n",
    "    self.para_encoder = ParaEncoderForContext(bilayers = encoder_bilayers, input_dim = encoder_input_dim, hidden_size = encoder_hidden_size)\n",
    "    self.para_decoder = ParaDecoderBiLstmCRF(input_dim = encoder_hidden_size*2, hidden_size = decoder_hidden_size, bilayers = decoder_bilayers)\n",
    "    # self.crf_model = crf_model\n",
    "    self.embed_model = embed_model\n",
    "    #DO NOT freeze bert embedding layer : Fine Tune BERT\n",
    "    # for param in self.embed_model.parameters():\n",
    "    #   param.requires_grad = False\n",
    "\n",
    "  def decode(self, emissions):\n",
    "    return self.crf_model.decode(emissions)\n",
    "\n",
    "  def forward(self, x): # (B, 2*context+1, tokens_per_para)\n",
    "    # print(\"Input to model: \", x.shape)\n",
    "    s0, s1, s2 = x.shape\n",
    "    xv = x.view(s0*s1, s2)\n",
    "    embeds = self.embed_model(xv)\n",
    "    para_vec = self.para_encoder(embeds)\n",
    "    pvv = para_vec.view(s0, s1, -1) #(B, T, 2*hidden_dim)\n",
    "    # print(\"Input to decoder: \", pvv.shape)\n",
    "    decoder_result = self.para_decoder(pvv) #(B,T,3) #emissions\n",
    "    return decoder_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxePIGPBZefb"
   },
   "source": [
    "## Train and Validate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "R661ZBw5TJ3v"
   },
   "outputs": [],
   "source": [
    "def train(para_model, crf_model, data_loader):\n",
    "  para_model.train()\n",
    "  # crf_model.train()\n",
    "    \n",
    "  avg_loss = []\n",
    "  # all_predictions = []\n",
    "  # all_targets = []\n",
    "  start = time.time()\n",
    "\n",
    "  for i, (x, y) in enumerate(tqdm(data_loader, desc=\"Epoch\", leave=False)):\n",
    "    optimizer.zero_grad()\n",
    "    y  = y.to(device) \n",
    "    x = x.to(device)\n",
    "\n",
    "    emission = para_model(x) \n",
    "    del x\n",
    "\n",
    "    log_likelihood = crf_model(emission, y, reduction='mean') \n",
    "    loss = -log_likelihood\n",
    "\n",
    "    avg_loss.extend([loss.item()]*len(y))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "   \n",
    "    del y\n",
    "    del emission\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "   \n",
    " \n",
    "    #do not decode during training to save time\n",
    "#     decoded_list = crf_model.decode(output)\n",
    "#     for l in decoded_list:\n",
    "#         all_predictions.extend(l)\n",
    "        \n",
    "#     all_targets.extend(torch.flatten(y).cpu().tolist())\n",
    "    \n",
    "    \n",
    "    \n",
    "  end = time.time()\n",
    "  avg_loss = np.mean(avg_loss)\n",
    "  print('learning_rate: {}'.format(scheduler.get_last_lr()))\n",
    "  print('Training loss: {:.2f}, Time: {}'.format(avg_loss, end-start))\n",
    "  \n",
    "#   all_predictions = np.array(all_predictions)\n",
    "#   # print(all_predictions.shape)\n",
    "#   all_targets = np.array(all_targets)\n",
    "#   scores = precision_recall_fscore_support(all_targets, all_predictions, \n",
    "#                                             average=\"weighted\", zero_division=0.)\n",
    "  \n",
    "#   test_scores={\n",
    "#       \"eval_accuracy\": (all_predictions == all_targets).sum() / len(all_predictions),\n",
    "#       \"eval_precision\": scores[0],\n",
    "#       \"eval_recall\": scores[1],\n",
    "#       \"eval_f-1\": scores[2]\n",
    "#   }\n",
    "#   print(test_scores)\n",
    "#   return test_scores[\"eval_f-1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_y3ZyfMCW6A8"
   },
   "outputs": [],
   "source": [
    "def validate(para_model, crf_model, data_loader):\n",
    "  para_model.eval()\n",
    "  # crf_model.eval()\n",
    "  \n",
    "  avg_loss = []\n",
    "  all_predictions = []\n",
    "  all_targets = []\n",
    "  start = time.time()\n",
    "\n",
    "  for i, (x, y) in enumerate(tqdm(data_loader, desc=\"Epoch\", leave=False)):\n",
    "    # optimizer.zero_grad()\n",
    "\n",
    "    y = y.to(device)\n",
    "    x = x.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      emissions = para_model(x)\n",
    "      del x\n",
    "      log_likelihood = crf_model(emissions, y)  #think of crf as softmax_cross_entropy_loss (activation + loss)\n",
    "      loss = -log_likelihood\n",
    "  \n",
    "      avg_loss.extend([loss.item()]*len(y))\n",
    "\n",
    "      decoded_list = crf_model.decode(emissions)\n",
    "      for l in decoded_list:\n",
    "        all_predictions.extend(l)\n",
    "      \n",
    "      all_targets.extend(torch.flatten(y).cpu().tolist())\n",
    "      del emissions\n",
    "      del y\n",
    "      torch.cuda.empty_cache()\n",
    "      \n",
    "  end = time.time()\n",
    "  avg_loss = np.mean(avg_loss)\n",
    "  print('learning_rate: {}'.format(scheduler.get_last_lr()))\n",
    "  print('Validation loss: {:.2f}, Time: {}'.format(avg_loss, end-start))\n",
    "  \n",
    "  all_predictions = np.array(all_predictions)\n",
    "  all_targets = np.array(all_targets)\n",
    "  scores = precision_recall_fscore_support(all_targets, all_predictions, \n",
    "                                            average=\"weighted\", zero_division=0.)\n",
    "  \n",
    "  test_scores={\n",
    "      \"eval_accuracy\": (all_predictions == all_targets).sum() / len(all_predictions),\n",
    "      \"eval_precision\": scores[0],\n",
    "      \"eval_recall\": scores[1],\n",
    "      \"eval_f-1\": scores[2]\n",
    "  }\n",
    "  print(test_scores)\n",
    "  return test_scores[\"eval_f-1\"], all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0bgq32FRSK3P"
   },
   "outputs": [],
   "source": [
    "def save(model, acc, best=\"\"):\n",
    "    if not os.path.exists('./bert_iob_bilstm_crf_bert_finetune/'):\n",
    "        os.mkdir('./bert_iob_bilstm_crf_bert_finetune/')\n",
    "\n",
    "    torch.save(model.state_dict(), './bert_iob_bilstm_crf_bert_finetune/'+'/{}model_params_{}.pth'.format(best, acc))\n",
    "\n",
    "def load_pretrained_weights(model, pretrained_path):\n",
    "    pretrained_dict = torch.load(pretrained_path)\n",
    "#     pretrained_dict = {k: v for k, v in pretrained_dict.items() if k[:13] == \"para_encoder.\"}\n",
    "    print(pretrained_dict.keys())\n",
    "    model_dict = model.state_dict()\n",
    "    model_dict.update(pretrained_dict) \n",
    "    model.load_state_dict(model_dict)\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKb52nsGZllu"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uN6Pfp1_YyVl",
    "outputId": "4d59b235-ca6e-411f-a607-91592b5a7906"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 7 GPUs!\n",
      "Total params:  130473219\n",
      "Trainable params:  130473219\n",
      "Non Trainable params:  0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "crf_model = CRF(num_tags = 3, batch_first = True)\n",
    "crf_model = crf_model.to(device)\n",
    "# model.load_state_dict(torch.load('./bert_iob_bilstm_crf/model_model_params_0.9428545098368426.pth'))\n",
    "# model = load_pretrained_weights(model, '/home/anjadhav/Chemical-Patent-Reaction-Extraction/model_model_params_0.9428545098368426.pth')\n",
    "\n",
    "model = EncoderDecoderBiLstmCRF(embed_model = BertEmbedding(pretrained_model))\n",
    "if torch.cuda.device_count() > 1:\n",
    "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "  model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "non_trainable_total_params = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "print(\"Total params: \", total_params)\n",
    "print(\"Trainable params: \", trainable_total_params)\n",
    "print(\"Non Trainable params: \", non_trainable_total_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Apmgr6gLTH3X"
   },
   "outputs": [],
   "source": [
    "epochs = 20 #changed from 10\n",
    "lamda = 1e-3  #L2 regularization (prev : 1e-4)\n",
    "learning_rate = 1e-2 #changed from 1e-2 \n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# criterion = criterion.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=lamda)\n",
    "# optimizer.load_state_dict(torch.load('./bert_base_triplet/optimizer_model_params_0.9409211846833226.pth'))    \n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[i for i in range(4,20,4)], gamma=0.75)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ApcG3QO1Vel2",
    "outputId": "b5cd2679-a0cd-4469-c595-341d5b6944ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-bc88f77b96c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch #{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrf_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mval_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mcrf_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-678e90ace115>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(para_model, crf_model, data_loader)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0memission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpara_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/chemIR/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/chemIR/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/chemIR/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/chemIR/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0m_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_tup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstreams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/chemIR/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/chemIR/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "best_val_f1 = 0\n",
    "for epoch in range(epochs):\n",
    "  print('Epoch #{}'.format(epoch+1))\n",
    "  \n",
    "  train(model, crf_model, train_loader)\n",
    "  val_f1, _ = validate(model,  crf_model, val_loader)\n",
    "  \n",
    "  if val_f1 > best_val_f1:\n",
    "    best_val_f1 = val_f1\n",
    "    save(model, best_val_f1, best = \"enc_dec_model_\")\n",
    "    save(crf_model, best_val_f1, best = \"crf_model_\")\n",
    "    save(optimizer, best_val_f1, best = \"optimizer_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymyNg6tRaCEi"
   },
   "outputs": [],
   "source": [
    "# Test on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predictions = validate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12871\n",
      "12864\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_file)\n",
    "print(len(test_df))\n",
    "test_df = test_df[:][:len(predictions)]\n",
    "test_df['predictions'] = predictions\n",
    "test_df.to_csv(\"bert_embed_iob_bilstm_crf_pred.csv\")\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'para', 'label', 'document', 'predictions'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Span Retrieval Results\n",
    "test_df = test_df.reset_index(drop=False)\n",
    "print(test_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.columns = ['index', 'para', 'label', 'document', 'predictions']\n",
    "\n",
    "orig = set()\n",
    "i = 0\n",
    "while i < len(test_df):\n",
    "    if(test_df['label'][i] == 2):\n",
    "        st = test_df['index'][i]\n",
    "        i +=1\n",
    "        while(i < len(test_df) and test_df['label'][i] == 1):\n",
    "            i+=1\n",
    "        orig.add((st, i-1))\n",
    "    else:\n",
    "        i+=1\n",
    "\n",
    "pred = set()\n",
    "i = 0\n",
    "while i < len(test_df):\n",
    "    if(test_df['predictions'][i] == 2):\n",
    "        st = test_df['index'][i]\n",
    "        i +=1\n",
    "        while(i < len(test_df) and test_df['predictions'][i] == 1):\n",
    "            i+=1\n",
    "        pred.add((st, i-1))\n",
    "    else:\n",
    "        i+=1\n",
    "        \n",
    "strict_match_spans = orig.intersection(pred)\n",
    "fuzzy_cnt = 0\n",
    "for o in orig:\n",
    "    if ((o in pred) or ((o[0]+1,o[1]) in pred) or ((o[0]+1,o[1]-1) in pred) or ((o[0]+1,o[1]+1) in pred) \n",
    "        or ((o[0]-1,o[1]) in pred) or ((o[0]-1,o[1]+1) in pred) or ((o[0]-1,o[1]-1) in pred) or ((o[0],o[1]+1) in pred)\n",
    "        or ((o[0],o[1]-1) in pred)):\n",
    "        fuzzy_cnt+=1\n",
    "  \n",
    "\n",
    "miss_start_end = 0\n",
    "miss_start = 0\n",
    "miss_end = 0\n",
    "\n",
    "for o in orig:\n",
    "    if(o in pred):\n",
    "        continue \n",
    "    elif(((o[0]-1,o[1]+1) in pred) or ((o[0]-1,o[1]-1) in pred) or ((o[0]+1,o[1]-1) in pred) or ((o[0]+1,o[1]+1) in pred)):\n",
    "        miss_start_end += 1\n",
    "    elif(((o[0]+1,o[1]) in pred) or ((o[0]-1,o[1]) in pred)):\n",
    "        miss_start += 1\n",
    "    elif(((o[0],o[1]+1) in pred) or ((o[0],o[1]-1) in pred)):\n",
    "        miss_end+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total original spans:  1234\n",
      "Total predicted spans:  1118\n",
      "Total number of original spans correctly predicted acc to strict match:  797\n",
      "Percent of original spans correctly predicted acc to strict match:  64.58670988654781\n",
      "Total number of original spans correctly predicted acc to fuzzy match:  955\n",
      "Percent of original spans correctly predicted acc to fuzzy match:  77.3905996758509\n",
      "Count of fuzzy matched spans:  158\n",
      "Count of spans with misaligned begin and end: 22 (13.92%) \n",
      "Count of spans with misaligned begin: 52 (32.91%) \n",
      "Count of spans with misaligned end: 84 (53.16%) \n"
     ]
    }
   ],
   "source": [
    "print(\"Total original spans: \", len(orig))\n",
    "print(\"Total predicted spans: \", len(pred))\n",
    "print(\"Total number of original spans correctly predicted acc to strict match: \", len(strict_match_spans))\n",
    "print(\"Percent of original spans correctly predicted acc to strict match: \", len(strict_match_spans)/len(orig)*100)\n",
    "\n",
    "print(\"Total number of original spans correctly predicted acc to fuzzy match: \", fuzzy_cnt)\n",
    "print(\"Percent of original spans correctly predicted acc to fuzzy match: \", fuzzy_cnt/len(orig)*100)\n",
    "\n",
    "fuzzy_matched_only = miss_start_end+miss_start+miss_end\n",
    "assert(fuzzy_matched_only == fuzzy_cnt - len(strict_match_spans))\n",
    "print(\"Count of fuzzy matched spans: \", miss_start_end+miss_start+miss_end)\n",
    "print(\"Count of spans with misaligned begin and end: {} ({:.2f}%) \".format(miss_start_end, miss_start_end/fuzzy_matched_only*100))\n",
    "print(\"Count of spans with misaligned begin: {} ({:.2f}%) \".format(miss_start, miss_start/fuzzy_matched_only*100))\n",
    "print(\"Count of spans with misaligned end: {} ({:.2f}%) \".format(miss_end, miss_end/fuzzy_matched_only*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store error cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = test_df[test_df['label'] != test_df['predictions']]\n",
    "print((len(test_df)- len(error)) / len(test_df))\n",
    "print(len(error))\n",
    "error.to_csv(\"errors_bert_embed_iob_bilstm_crf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('chemIR': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bca01836d6106e973a10514ba84a22def982a0caf4526f342e9e0f31ef9581b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
